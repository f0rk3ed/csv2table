#!/usr/bin/env python

import argparse
import csv
import datetime
import decimal
import gzip
import json
import os
import re
import signal
import subprocess
import sys
from collections import defaultdict
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union
from urllib.parse import urlparse

# Ignore pipe errors and set CSV field size limit
signal.signal(signal.SIGPIPE, signal.SIG_DFL)
csv.field_size_limit(sys.maxsize)


class SQLType(Enum):
    TEXT = "text"
    INTEGER = "integer"
    BIGINTEGER = "biginteger"
    NUMERIC = "numeric"
    DATE = "date"
    TIMESTAMP = "timestamp"
    TIMESTAMPTZ = "timestamptz"
    JSON = "json"
    JSONB = "jsonb"
    EMPTY = None


@dataclass
class RedshiftConfig:
    credentials_path: Optional[str] = None
    bucket: Optional[str] = None
    upload: bool = False
    
    def __post_init__(self):
        if self.upload and not self.credentials_path:
            self.credentials_path = "<ENV>"


@dataclass
class Config:
    file_path: str
    table_name: Optional[str] = None
    schema: Optional[str] = None
    delimiter: str = ","
    quote_char: str = '"'
    
    # Type detection
    detect_integers: bool = False
    detect_numeric: bool = False
    detect_dates: bool = False
    detect_timestamps: bool = False
    detect_json: bool = False
    detect_jsonb: bool = False
    detect_arrays: bool = False
    use_big_integers: bool = False
    use_timestamptz: bool = False
    
    # Processing options
    clean_names: bool = False
    lowercase_names: bool = False
    drop_table: bool = False
    truncate_table: bool = False
    create_schema: bool = False
    no_create: bool = False
    temporary_table: bool = False
    create_if_not_exists: bool = False
    fix_duplicates: bool = False
    missing_headers: bool = False
    is_gzipped: bool = False
    transaction: bool = False
    
    # Copy options
    generate_copy: bool = False
    use_backslash_copy: bool = False
    
    # Analysis options
    sample_rows: Optional[int] = None
    skip_parsing_cols: List[str] = field(default_factory=list)
    parse_only_cols: Optional[List[str]] = None
    
    # Additional columns
    file_column: Optional[str] = None
    serial_column: Optional[str] = None
    primary_key: Optional[str] = None
    
    # Redshift support
    redshift: Optional[RedshiftConfig] = None


@dataclass
class Column:
    name: str
    sql_type: SQLType
    is_array: bool = False


class TypeDetector:
    # Date/time format mappings for Redshift
    DATE_FORMATS = {
        "%Y-%m-%d": "YYYY-MM-DD",
        "%Y%m%d": "YYYYMMDD", 
        "%m/%d/%Y": "MM/DD/YYYY",
        "%b %d, %Y": "MON DD, YYYY",
        "%m/%d/%y": "MM/DD/YY"
    }
    
    TIME_FORMATS = {
        "%H:%M:%S": "HH:MM:SS",
        "%H:%M:%S %z": "HH:MM:SS",
        "%I:%M:%S %p": "HH12:MM:SS AM",
        "%H:%M:%S.%f": "HH:MM:SS",
        "%H:%M": "HH:MM"
    }
    
    def __init__(self, config: Config):
        self.config = config
        self.redshift_date_format = "YYYY-MM-DD"
        self.redshift_datetime_format = "YYYY-MM-DD HH:MM:SS"
        
        # Adjust numeric type for Redshift
        if config.redshift:
            SQLType.NUMERIC._value_ = "numeric(24, 8)"
    
    def detect_type(self, value: str) -> Tuple[SQLType, bool]:
        if not value.strip():
            return SQLType.EMPTY, False
        
        # Array detection first
        if self.config.detect_arrays and self._is_array(value):
            return self._detect_array_type(value)
        
        # Date detection
        if self.config.detect_dates:
            for fmt, redshift_fmt in self.DATE_FORMATS.items():
                if self._try_parse_datetime(value, fmt):
                    self.redshift_date_format = redshift_fmt
                    return SQLType.DATE, False
        
        # Timestamp detection
        if self.config.detect_timestamps:
            for date_fmt, rs_date_fmt in self.DATE_FORMATS.items():
                if self._try_parse_datetime(value, date_fmt):
                    self.redshift_datetime_format = rs_date_fmt
                    return (SQLType.TIMESTAMPTZ if self.config.use_timestamptz 
                           else SQLType.TIMESTAMP), False
                
                for time_fmt, rs_time_fmt in self.TIME_FORMATS.items():
                    for sep in [" ", "T"]:
                        full_fmt = f"{date_fmt}{sep}{time_fmt}"
                        if self._try_parse_datetime(value, full_fmt):
                            self.redshift_datetime_format = f"{rs_date_fmt} {rs_time_fmt}"
                            return (SQLType.TIMESTAMPTZ if self.config.use_timestamptz 
                                   else SQLType.TIMESTAMP), False
        
        # Integer detection
        if self.config.detect_integers and self._is_integer(value):
            return (SQLType.BIGINTEGER if self.config.use_big_integers 
                   else SQLType.INTEGER), False
        
        # Numeric detection
        if self.config.detect_numeric and self._is_numeric(value):
            return SQLType.NUMERIC, False
        
        # JSON detection
        if (self.config.detect_json or self.config.detect_jsonb) and self._is_json(value):
            return (SQLType.JSONB if self.config.detect_jsonb else SQLType.JSON), False
        
        return SQLType.TEXT, False
    
    def _is_array(self, value: str) -> bool:
        return value.startswith("{") and value.endswith("}")
    
    def _try_parse_datetime(self, value: str, fmt: str) -> bool:
        try:
            datetime.datetime.strptime(value, fmt)
            return len(value) == 8 if fmt == "%Y%m%d" else True
        except ValueError:
            return False
    
    def _is_integer(self, value: str) -> bool:
        try:
            int(value)
            return True
        except ValueError:
            return False
    
    def _is_numeric(self, value: str) -> bool:
        try:
            decimal.Decimal(value)
            return True
        except decimal.InvalidOperation:
            return False
    
    def _is_json(self, value: str) -> bool:
        if not ((value.startswith("{") and value.endswith("}")) or 
                (value.startswith("[") and value.endswith("]"))):
            return False
        try:
            json.loads(value)
            return True
        except (ValueError, json.JSONDecodeError):
            return False
    
    def _detect_array_type(self, value: str) -> Tuple[SQLType, bool]:
        try:
            # Parse array content
            inner = value[1:-1].strip()
            if not inner:
                return SQLType.TEXT, True
            
            # Simple CSV parsing for array elements
            reader = csv.reader([inner])
            elements = next(reader, [])
            
            if not elements:
                return SQLType.TEXT, True
            
            # Detect type of first non-empty element
            for elem in elements:
                if elem.strip():
                    elem_type, _ = self.detect_type(elem.strip())
                    return elem_type, True
            
            return SQLType.TEXT, True
        except Exception:
            return SQLType.TEXT, False


class RedshiftManager:
    def __init__(self, config: RedshiftConfig):
        self.config = config
        self.aws_access_key_id = None
        self.aws_secret_access_key = None
        self.s3_bucket = None
        self._load_credentials()
    
    def _load_credentials(self):
        if self.config.credentials_path == "<ENV>":
            self._load_from_env()
        elif self.config.credentials_path:
            self._load_from_file(self.config.credentials_path)
        
        if self.config.bucket:
            self.s3_bucket = self.config.bucket
        
        self._validate_credentials()
    
    def _load_from_env(self):
        self.aws_access_key_id = os.getenv("AWS_ACCESS_KEY_ID")
        self.aws_secret_access_key = os.getenv("AWS_SECRET_ACCESS_KEY")
        self.s3_bucket = os.getenv("S3_BUCKET")
        
        # Try AWS credential file if env vars not found
        if not self.aws_access_key_id:
            cred_file = os.getenv("AWS_CREDENTIAL_FILE")
            if cred_file:
                self._load_from_file(cred_file)
    
    def _load_from_file(self, path: str):
        if not os.path.exists(path):
            return
        
        patterns = {
            'access_key': r'(AWSAccessKeyId|aws_access_key_id|access_key|s3\.account_id)\s*=\s*(.+)$',
            'secret_key': r'(AWSSecretKey|aws_secret_access_key|secret_key|s3\.private_key)\s*=\s*(.+)$',
            'bucket': r's3\.bucket\s*=\s*(.+)$',
            'transfer_bucket': r's3\.transfer_bucket\s*=\s*(.+)$'
        }
        
        with open(path) as f:
            for line in f:
                if line.startswith("#"):
                    continue
                
                for key, pattern in patterns.items():
                    match = re.search(pattern, line)
                    if match:
                        value = match.group(2 if key in ['access_key', 'secret_key'] else 1).strip()
                        if key == 'access_key':
                            self.aws_access_key_id = value
                        elif key == 'secret_key':
                            self.aws_secret_access_key = value
                        elif key == 'transfer_bucket':
                            self.s3_bucket = value  # Transfer bucket takes precedence
                        elif key == 'bucket' and not self.s3_bucket:
                            self.s3_bucket = value
    
    def _validate_credentials(self):
        if not all([self.aws_access_key_id, self.aws_secret_access_key, self.s3_bucket]):
            missing = []
            if not self.aws_access_key_id:
                missing.append("aws_access_key_id")
            if not self.aws_secret_access_key:
                missing.append("aws_secret_access_key")
            if not self.s3_bucket:
                missing.append("s3_bucket")
            raise ValueError(f"Missing Redshift credentials: {', '.join(missing)}")
    
    def get_s3_path(self, file_path: str) -> str:
        s3_url = f"s3://{os.path.join(self.s3_bucket, os.path.basename(file_path))}"
        return s3_url
    
    def upload_file(self, file_path: str):
        if not self.config.upload:
            return
        
        try:
            import botocore.session
        except ImportError:
            raise ImportError("botocore required for Redshift upload")
        
        s3_path = self.get_s3_path(file_path)
        parsed = urlparse(s3_path)
        bucket = parsed.netloc
        key = parsed.path.lstrip("/")
        
        session = botocore.session.get_session()
        client = session.create_client(
            service_name="s3",
            aws_access_key_id=self.aws_access_key_id,
            aws_secret_access_key=self.aws_secret_access_key,
        )
        
        with open(file_path, "rb") as f:
            client.put_object(Bucket=bucket, Key=key, Body=f)


class NameCleaner:
    def __init__(self, config: Config):
        self.config = config
        self.empty_counter = 0
    
    def clean(self, name: str) -> str:
        name = self._strip_bom(name)
        
        if not self.config.clean_names:
            return name.replace('"', '')
        
        name = name.strip()
        name = re.sub(r'[^a-zA-Z0-9_]', '_', name)
        name = re.sub(r'_+', '_', name).strip('_')
        
        if self.config.lowercase_names:
            name = name.lower()
        
        if not name:
            self.empty_counter += 1
            name = "_" * self.empty_counter
        
        return name
    
    @staticmethod
    def _strip_bom(field: str) -> str:
        if field.startswith('\ufeff'):
            return field[1:]
        try:
            if field.encode('utf-8')[:3] == b'\xef\xbb\xbf':
                return field[3:]
        except (UnicodeEncodeError, AttributeError):
            pass
        return field


class CSVAnalyzer:
    TYPE_UPGRADES = {
        SQLType.INTEGER: [SQLType.NUMERIC],
        SQLType.BIGINTEGER: [SQLType.NUMERIC],
        SQLType.DATE: [SQLType.TIMESTAMP, SQLType.TIMESTAMPTZ],
    }
    
    def __init__(self, config: Config):
        self.config = config
        self.detector = TypeDetector(config)
        self.cleaner = NameCleaner(config)
        self.seen_names = defaultdict(int)
    
    def analyze(self) -> List[Column]:
        # Handle gzipped files
        if self.config.is_gzipped:
            process = subprocess.Popen(
                ["zcat", self.config.file_path],
                stdout=subprocess.PIPE,
                universal_newlines=True,
            )
            fp = process.stdout
        else:
            fp = open(self.config.file_path, "r", newline='')
        
        try:
            reader = csv.DictReader(
                fp, 
                delimiter=self.config.delimiter,
                quotechar=self.config.quote_char
            )
            
            columns = []
            for row_idx, row in enumerate(reader):
                if not columns:
                    columns = self._init_columns(reader.fieldnames)
                
                self._update_columns(columns, row, reader.fieldnames)
                
                if (self.config.sample_rows and 
                    row_idx >= self.config.sample_rows):
                    break
            
            return columns
        finally:
            if self.config.is_gzipped:
                process.kill()
            else:
                fp.close()
    
    def _init_columns(self, fieldnames: List[str]) -> List[Column]:
        if self.config.file_column:
            self.seen_names[self.config.file_column] = 0
        
        columns = []
        for i, field in enumerate(fieldnames):
            if self.config.missing_headers:
                name = f"col_{i}"
            else:
                name = self._get_unique_name(field)
            columns.append(Column(name, SQLType.EMPTY))
        
        return columns
    
    def _get_unique_name(self, field: str) -> str:
        name = self.cleaner.clean(field)
        
        if self.config.fix_duplicates and name in self.seen_names:
            self.seen_names[name] += 1
            name = f"{name}_{self.seen_names[name]}"
        else:
            self.seen_names[name] = 0
        
        return name
    
    def _update_columns(self, columns: List[Column], row: Dict, fieldnames: List[str]):
        for i, field in enumerate(fieldnames):
            value = row[field]
            
            if self._should_skip_parsing(field, columns[i].name):
                new_type = SQLType.TEXT if value else SQLType.EMPTY
                is_array = False
            else:
                new_type, is_array = self.detector.detect_type(value)
            
            columns[i] = self._merge_types(columns[i], new_type, is_array)
    
    def _should_skip_parsing(self, field: str, clean_name: str) -> bool:
        if self.config.parse_only_cols:
            return field not in self.config.parse_only_cols
        return (field in self.config.skip_parsing_cols or 
                clean_name in self.config.skip_parsing_cols)
    
    def _merge_types(self, column: Column, new_type: SQLType, is_array: bool) -> Column:
        if column.sql_type == SQLType.EMPTY:
            return Column(column.name, new_type, is_array)
        
        if new_type == SQLType.EMPTY:
            return column
        
        if column.is_array != is_array:
            return Column(column.name, SQLType.TEXT, False)
        
        if column.sql_type == new_type:
            return column
        
        # Check upgrades
        if (column.sql_type in self.TYPE_UPGRADES and 
            new_type in self.TYPE_UPGRADES[column.sql_type]):
            return Column(column.name, new_type, is_array)
        
        if (new_type in self.TYPE_UPGRADES and 
            column.sql_type in self.TYPE_UPGRADES[new_type]):
            return column
        
        return Column(column.name, SQLType.TEXT, False)


class SQLGenerator:
    def __init__(self, config: Config):
        self.config = config
        self.redshift_manager = None
        if config.redshift:
            self.redshift_manager = RedshiftManager(config.redshift)
    
    def generate(self, columns: List[Column]) -> str:
        statements = []
        
        if self.config.transaction:
            statements.append("BEGIN;")
        
        if self.config.create_schema and self.config.schema:
            statements.append(f"CREATE SCHEMA {self._quote(self.config.schema)};")
        
        table_name = self._get_table_name()
        full_name = self._get_full_table_name(table_name)
        
        if self.config.drop_table and not self.config.no_create:
            drop_sql = "DROP TABLE {};"
            if not self.config.redshift:
                drop_sql = "DROP TABLE IF EXISTS {};"
            statements.append(drop_sql.format(full_name))
        
        if not self.config.no_create:
            statements.append(self._create_table_sql(full_name, columns))
        
        if self.config.truncate_table:
            statements.append(f"TRUNCATE TABLE {full_name};")
        
        if self.config.generate_copy:
            statements.append(self._copy_sql(full_name, columns))
        
        if self.config.transaction:
            statements.append("COMMIT;")
        
        # Handle Redshift upload
        if self.redshift_manager and self.config.redshift.upload:
            self.redshift_manager.upload_file(self.config.file_path)
        
        return "\n".join(statements)
    
    def _get_table_name(self) -> str:
        if self.config.table_name:
            return self.config.table_name
        
        # Extract table name from file path
        path = Path(self.config.file_path)
        name = path.stem
        if path.suffix == '.gz':
            name = Path(name).stem
        
        return self.cleaner.clean(name) if self.config.clean_names else name
    
    def _get_full_table_name(self, table_name: str) -> str:
        if self.config.schema:
            return f"{self._quote(self.config.schema)}.{self._quote(table_name)}"
        return self._quote(table_name)
    
    def _create_table_sql(self, full_name: str, columns: List[Column]) -> str:
        create_stmt = "CREATE"
        if self.config.temporary_table:
            create_stmt += " TEMPORARY"
        create_stmt += " TABLE"
        if self.config.create_if_not_exists:
            create_stmt += " IF NOT EXISTS"
        
        lines = [f"{create_stmt} {full_name} ("]
        
        # Add serial column
        if self.config.serial_column:
            lines.append(f"    {self._quote(self.config.serial_column)} SERIAL,")
        
        # Add file column
        if self.config.file_column:
            escaped = self.config.file_path.replace("'", "''")
            lines.append(f"    {self._quote(self.config.file_column)} TEXT DEFAULT '{escaped}',")
        
        # Add data columns
        for column in columns:
            sql_type = column.sql_type.value if column.sql_type != SQLType.EMPTY else "text"
            type_str = f"{sql_type}{'[]' if column.is_array else ''}"
            lines.append(f"    {self._quote(column.name)} {type_str},")
        
        # Remove trailing comma
        if lines[-1].endswith(','):
            lines[-1] = lines[-1][:-1]
        
        # Add primary key
        if self.config.primary_key:
            pkey_cols = [col.strip() for col in self.config.primary_key.split(",")]
            quoted_cols = [self._quote(col) for col in pkey_cols if col]
            if self.config.serial_column:
                quoted_cols.insert(0, self._quote(self.config.serial_column))
            lines.append(f"    PRIMARY KEY ({', '.join(quoted_cols)})")
        
        lines.append(");")
        return "\n".join(lines)
    
    def _copy_sql(self, full_name: str, columns: List[Column]) -> str:
        targets = ", ".join(self._quote(col.name) for col in columns)
        copy_cmd = r"\copy" if self.config.use_backslash_copy else "COPY"
        
        if self.config.redshift and self.redshift_manager:
            # Redshift COPY command
            s3_path = self.redshift_manager.get_s3_path(self.config.file_path)
            credentials = (f"aws_access_key_id={self.redshift_manager.aws_access_key_id};"
                          f"aws_secret_access_key={self.redshift_manager.aws_secret_access_key}")
            
            options = (f"'{s3_path}' WITH CREDENTIALS '{credentials}' "
                      f"CSV IGNOREHEADER 1 DELIMITER '{self._escape(self.config.delimiter)}' "
                      f"QUOTE '{self._escape(self.config.quote_char)}'")
            
            if self.config.is_gzipped:
                options += " GZIP"
            
            # Add date/time formats if detected
            detector = self.detector if hasattr(self, 'detector') else TypeDetector(self.config)
            options += f" DATEFORMAT '{detector.redshift_date_format}'"
            options += f" TIMEFORMAT '{detector.redshift_datetime_format}'"
            
        else:
            # PostgreSQL COPY command
            if self.config.is_gzipped:
                source = f"PROGRAM 'gunzip < {self._escape(self.config.file_path)}'"
            else:
                source = f"'{self._escape(self.config.file_path)}'"
            
            options = (f"{source} WITH CSV HEADER "
                      f"DELIMITER '{self._escape(self.config.delimiter)}' "
                      f"QUOTE '{self._escape(self.config.quote_char)}'")
        
        return f"{copy_cmd} {full_name}({targets}) FROM {options};"
    
    @staticmethod
    def _quote(name: str) -> str:
        return f'"{name.replace('"', '""')}"'
    
    @staticmethod 
    def _escape(value: str) -> str:
        return value.replace("'", "''")


def create_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Generate PostgreSQL/Redshift schema from CSV")
    
    parser.add_argument("-f", "--file", required=True, help="CSV file path")
    parser.add_argument("-t", "--table", help="Table name")
    parser.add_argument("--schema", help="Schema name")
    parser.add_argument("--create-schema", action="store_true", help="Create schema")
    
    # CSV options
    parser.add_argument("-d", "--delimiter", default=",", help="CSV delimiter")
    parser.add_argument("-q", "--quote", default='"', help="Quote character")
    parser.add_argument("--gzip", action="store_true", help="Gzipped file")
    parser.add_argument("--missing-headers", action="store_true", help="Generate column names")
    
    # Type detection
    parser.add_argument("-i", "--integer", action="store_true", help="Detect integers")
    parser.add_argument("-n", "--numeric", action="store_true", help="Detect numeric")
    parser.add_argument("-p", "--timestamp", action="store_true", help="Detect timestamps")
    parser.add_argument("--date", action="store_true", help="Detect dates")
    parser.add_argument("--tz", action="store_true", help="Use timestamptz")
    parser.add_argument("-b", "--big-integer", action="store_true", help="Use bigint")
    parser.add_argument("--json", action="store_true", help="Detect JSON")
    parser.add_argument("--jsonb", action="store_true", help="Detect JSONB")
    parser.add_argument("--array", action="store_true", help="Detect arrays")
    
    # Name processing
    parser.add_argument("-m", "--mogrify", action="store_true", help="Clean names")
    parser.add_argument("-l", "--lower", action="store_true", help="Lowercase names")
    parser.add_argument("--fix-duplicates", action="store_true", help="Handle duplicates")
    
    # Table options
    parser.add_argument("-x", "--drop", action="store_true", help="Drop table first")
    parser.add_argument("--truncate", action="store_true", help="Truncate table")
    parser.add_argument("-z", "--no-create", action="store_true", help="Skip CREATE TABLE")
    parser.add_argument("--temporary", action="store_true", help="Temporary table")
    parser.add_argument("--cine", action="store_true", help="CREATE IF NOT EXISTS")
    
    # Copy options
    parser.add_argument("-y", "--copy", action="store_true", help="Generate COPY")
    parser.add_argument("-s", "--backslash", action="store_true", help="Use \\copy")
    
    # Analysis
    parser.add_argument("--keep-going", type=int, help="Analyze N rows")
    parser.add_argument("--skip-parsing", help="Skip parsing columns")
    parser.add_argument("--parse-only", help="Parse only these columns")
    
    # Additional columns
    parser.add_argument("--file-column", help="Add filename column")
    parser.add_argument("--serial-column", help="Add serial column")
    parser.add_argument("--primary-key", help="Primary key columns")
    
    # Transaction
    parser.add_argument("-1", "--transaction", action="store_true", help="Use transaction")
    
    # Redshift options
    parser.add_argument("--redshift", help="Redshift credentials file or <ENV>")
    parser.add_argument("--redshift-bucket", help="S3 bucket override")
    parser.add_argument("--redshift-upload", action="store_true", help="Upload to S3")
    
    return parser


def main():
    parser = create_parser()
    args = parser.parse_args()
    
    # Validation
    if not os.path.exists(args.file):
        sys.stderr.write(f"File '{args.file}' does not exist.\n")
        sys.exit(1)
    
    if args.create_schema and not args.schema:
        sys.stderr.write("--create-schema requires --schema\n")
        sys.exit(1)
    
    if args.backslash and args.redshift:
        sys.stderr.write("--backslash incompatible with --redshift\n")
        sys.exit(1)
    
    # Handle special delimiters
    delimiter = args.delimiter
    if delimiter.lower() in ('\\t', '\\\\t', 'tab'):
        delimiter = '\t'
    
    # Build configuration
    redshift_config = None
    if args.redshift or args.redshift_upload:
        redshift_config = RedshiftConfig(
            credentials_path=args.redshift,
            bucket=args.redshift_bucket,
            upload=args.redshift_upload
        )
    
    config = Config(
        file_path=args.file,
        table_name=args.table,
        schema=args.schema,
        delimiter=delimiter,
        quote_char=args.quote,
        detect_integers=args.integer,
        detect_numeric=args.numeric,
        detect_dates=args.date,
        detect_timestamps=args.timestamp,
        detect_json=args.json,
        detect_jsonb=args.jsonb,
        detect_arrays=args.array,
        use_big_integers=args.big_integer,
        use_timestamptz=args.tz,
        clean_names=args.mogrify,
        lowercase_names=args.lower,
        drop_table=args.drop,
        truncate_table=args.truncate,
        create_schema=args.create_schema,
        no_create=args.no_create,
        temporary_table=args.temporary,
        create_if_not_exists=args.cine,
        fix_duplicates=args.fix_duplicates,
        missing_headers=args.missing_headers,
        is_gzipped=args.gzip,
        transaction=args.transaction,
        generate_copy=args.copy,
        use_backslash_copy=args.backslash,
        sample_rows=args.keep_going,
        skip_parsing_cols=[c.strip() for c in (args.skip_parsing or "").split(",") if c.strip()],
        parse_only_cols=[c.strip() for c in (args.parse_only or "").split(",") if c.strip()] if args.parse_only else None,
        file_column=args.file_column,
        serial_column=args.serial_column,
        primary_key=args.primary_key,
        redshift=redshift_config
    )
    
    try:
        # Analyze CSV and generate SQL
        analyzer = CSVAnalyzer(config)
        columns = analyzer.analyze()
        
        generator = SQLGenerator(config)
        sql = generator.generate(columns)
        
        print(sql)
        
    except Exception as e:
        sys.stderr.write(f"Error: {e}\n")
        sys.exit(1)


if __name__ == "__main__":
    main()
